{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BXRFbtgzLgPl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "neK1L3H7LpnJ"
   },
   "outputs": [],
   "source": [
    "# Create data set object\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zi5CMYEuL04q"
   },
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "iris_df = pd.DataFrame(data = np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DBiEQXAvMHHS",
    "outputId": "fbdc26dc-af20-4e66-ccc9-4a421f06c396"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
       "0                5.1               3.5  ...               0.2     0.0\n",
       "1                4.9               3.0  ...               0.2     0.0\n",
       "2                4.7               3.2  ...               0.2     0.0\n",
       "3                4.6               3.1  ...               0.2     0.0\n",
       "4                5.0               3.6  ...               0.2     0.0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "53ma_vwYNAa8"
   },
   "outputs": [],
   "source": [
    "# get features and labels from dataframe\n",
    "X = iris_df.drop(\"target\",axis=1).values\n",
    "y = iris_df[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xbOhf9KsNj1J"
   },
   "outputs": [],
   "source": [
    "# split training and testing data set 70:30 according to the rule of thumb that Aj said\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=42\n",
    "                                                    ,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "IcFuk5dzNpjT"
   },
   "outputs": [],
   "source": [
    "# Convert to tensor \n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "dqPT5lm1Nq9D"
   },
   "outputs": [],
   "source": [
    "# creating the neural network with 4 input nodes, 2 hidden layers (6 nodes for each)\n",
    "# and 3 output nodes (according to number of classes)\n",
    "# relu as activation function\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_features=4, hidden_layer1=6, hidden_layer2=6, output_features=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_features,hidden_layer1)                  \n",
    "        self.fc2 = nn.Linear(hidden_layer1, hidden_layer2)                  \n",
    "        self.out = nn.Linear(hidden_layer2, output_features)      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOG4D2EcN-KJ",
    "outputId": "23368be2-b7eb-4899-e462-c33ec95967ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=4, out_features=6, bias=True)\n",
       "  (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
       "  (out): Linear(in_features=6, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model object\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "x-CTvsraN_jr"
   },
   "outputs": [],
   "source": [
    "# CrossEntropy as loss function\n",
    "# use Adam as optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7g3ZNTRPOA9Z",
    "outputId": "6b5d7e98-7f2d-4d82-d0d9-39cc8a9d96b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss: 1.13593137\n",
      "epoch:  1  loss: 1.11570930\n",
      "epoch:  2  loss: 1.10180485\n",
      "epoch:  3  loss: 1.09254837\n",
      "epoch:  4  loss: 1.08648551\n",
      "epoch:  5  loss: 1.08437192\n",
      "epoch:  6  loss: 1.08494508\n",
      "epoch:  7  loss: 1.08480799\n",
      "epoch:  8  loss: 1.08294237\n",
      "epoch:  9  loss: 1.08034635\n",
      "epoch: 10  loss: 1.07817090\n",
      "epoch: 11  loss: 1.07678223\n",
      "epoch: 12  loss: 1.07564640\n",
      "epoch: 13  loss: 1.07389975\n",
      "epoch: 14  loss: 1.07109010\n",
      "epoch: 15  loss: 1.06735170\n",
      "epoch: 16  loss: 1.06311202\n",
      "epoch: 17  loss: 1.05869329\n",
      "epoch: 18  loss: 1.05414343\n",
      "epoch: 19  loss: 1.04906797\n",
      "epoch: 20  loss: 1.04304397\n",
      "epoch: 21  loss: 1.03570271\n",
      "epoch: 22  loss: 1.02784264\n",
      "epoch: 23  loss: 1.01964664\n",
      "epoch: 24  loss: 1.01061225\n",
      "epoch: 25  loss: 1.00029480\n",
      "epoch: 26  loss: 0.98867732\n",
      "epoch: 27  loss: 0.97601706\n",
      "epoch: 28  loss: 0.96251881\n",
      "epoch: 29  loss: 0.94779998\n",
      "epoch: 30  loss: 0.93160081\n",
      "epoch: 31  loss: 0.91423827\n",
      "epoch: 32  loss: 0.89617616\n",
      "epoch: 33  loss: 0.87688214\n",
      "epoch: 34  loss: 0.85599065\n",
      "epoch: 35  loss: 0.83316761\n",
      "epoch: 36  loss: 0.80821598\n",
      "epoch: 37  loss: 0.78134078\n",
      "epoch: 38  loss: 0.75372821\n",
      "epoch: 39  loss: 0.72698504\n",
      "epoch: 40  loss: 0.70139611\n",
      "epoch: 41  loss: 0.67716670\n",
      "epoch: 42  loss: 0.65271735\n",
      "epoch: 43  loss: 0.62772179\n",
      "epoch: 44  loss: 0.60313022\n",
      "epoch: 45  loss: 0.57886231\n",
      "epoch: 46  loss: 0.55598354\n",
      "epoch: 47  loss: 0.53525060\n",
      "epoch: 48  loss: 0.51551896\n",
      "epoch: 49  loss: 0.49582589\n",
      "epoch: 50  loss: 0.47682500\n",
      "epoch: 51  loss: 0.45782602\n",
      "epoch: 52  loss: 0.43911690\n",
      "epoch: 53  loss: 0.42142227\n",
      "epoch: 54  loss: 0.40397274\n",
      "epoch: 55  loss: 0.38671720\n",
      "epoch: 56  loss: 0.36961600\n",
      "epoch: 57  loss: 0.35255861\n",
      "epoch: 58  loss: 0.33584276\n",
      "epoch: 59  loss: 0.31995341\n",
      "epoch: 60  loss: 0.30463597\n",
      "epoch: 61  loss: 0.28989810\n",
      "epoch: 62  loss: 0.27590355\n",
      "epoch: 63  loss: 0.26270655\n",
      "epoch: 64  loss: 0.25045365\n",
      "epoch: 65  loss: 0.23918951\n",
      "epoch: 66  loss: 0.22871165\n",
      "epoch: 67  loss: 0.21910024\n",
      "epoch: 68  loss: 0.21028185\n",
      "epoch: 69  loss: 0.20182535\n",
      "epoch: 70  loss: 0.19364978\n",
      "epoch: 71  loss: 0.18575840\n",
      "epoch: 72  loss: 0.17799897\n",
      "epoch: 73  loss: 0.17066531\n",
      "epoch: 74  loss: 0.16373341\n",
      "epoch: 75  loss: 0.15720227\n",
      "epoch: 76  loss: 0.15118265\n",
      "epoch: 77  loss: 0.14549524\n",
      "epoch: 78  loss: 0.14023708\n",
      "epoch: 79  loss: 0.13530532\n",
      "epoch: 80  loss: 0.13067964\n",
      "epoch: 81  loss: 0.12638995\n",
      "epoch: 82  loss: 0.12234673\n",
      "epoch: 83  loss: 0.11861211\n",
      "epoch: 84  loss: 0.11510529\n",
      "epoch: 85  loss: 0.11184991\n",
      "epoch: 86  loss: 0.10884293\n",
      "epoch: 87  loss: 0.10602848\n",
      "epoch: 88  loss: 0.10344458\n",
      "epoch: 89  loss: 0.10104644\n",
      "epoch: 90  loss: 0.09883627\n",
      "epoch: 91  loss: 0.09679028\n",
      "epoch: 92  loss: 0.09488611\n",
      "epoch: 93  loss: 0.09312407\n",
      "epoch: 94  loss: 0.09147586\n",
      "epoch: 95  loss: 0.08995599\n",
      "epoch: 96  loss: 0.08853229\n",
      "epoch: 97  loss: 0.08721291\n",
      "epoch: 98  loss: 0.08597984\n",
      "epoch: 99  loss: 0.08482990\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training neural networks\n",
    "epochs = 100\n",
    "losses = []\n",
    "preds = []\n",
    "for i in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients on each training pass\n",
    "    loss.backward() # loss.backward() to calculate the gradients\n",
    "    optimizer.step() # update the weight\n",
    "\n",
    "# do not auto_grad for testing set due to we don't want to update weight\n",
    "with torch.no_grad(): \n",
    "  for val in X_test:\n",
    "      y_hat = model.forward(val)\n",
    "      preds.append(y_hat.argmax().item())\n",
    "\n",
    "plt.plot(range(1,epochs+1), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vi04XgcXOEGU",
    "outputId": "78ba8101-9411-42d5-921f-25ccc1ad546e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>YHat</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y  YHat  Correct\n",
       "0   1     1        1\n",
       "1   0     0        1\n",
       "2   2     2        1\n",
       "3   1     1        1\n",
       "4   1     1        1\n",
       "5   0     0        1\n",
       "6   1     1        1\n",
       "7   2     2        1\n",
       "8   1     1        1\n",
       "9   1     1        1\n",
       "10  2     2        1\n",
       "11  0     0        1\n",
       "12  0     0        1\n",
       "13  0     0        1\n",
       "14  0     0        1\n",
       "15  1     1        1\n",
       "16  2     2        1\n",
       "17  1     1        1\n",
       "18  1     1        1\n",
       "19  2     2        1\n",
       "20  0     0        1\n",
       "21  2     2        1\n",
       "22  0     0        1\n",
       "23  2     2        1\n",
       "24  2     2        1\n",
       "25  2     2        1\n",
       "26  2     2        1\n",
       "27  2     2        1\n",
       "28  0     0        1\n",
       "29  0     0        1\n",
       "30  0     0        1\n",
       "31  0     0        1\n",
       "32  1     1        1\n",
       "33  0     0        1\n",
       "34  0     0        1\n",
       "35  2     2        1\n",
       "36  1     1        1\n",
       "37  0     0        1\n",
       "38  0     0        1\n",
       "39  0     0        1\n",
       "40  2     2        1\n",
       "41  1     1        1\n",
       "42  1     1        1\n",
       "43  0     0        1\n",
       "44  0     0        1"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe to see the predicted output and actual output\n",
    "df = pd.DataFrame({'Y': y_test, 'YHat': preds})\n",
    "df['Correct'] = [1 if corr == pred else 0 for corr, pred in zip(df['Y'], df['YHat'])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBFW_IgtOFoz",
    "outputId": "010347c8-c7a6-4165-a1a5-6d08c5cfc5b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Correct'].sum() / len(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment1_iris_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
